{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75cbacf-589d-4e63-a88f-a950cf06dd38",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9a827-32da-4fe8-a0e1-02b673ccdc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from transformers import XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a436f-255f-41e4-b9d8-68d44e21c655",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1822db7e-7daa-4edc-8c86-3c8fc6ba847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\torchdata\\datapipes\\__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "train_iter,valid_iter=IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793912c8-95ad-40c9-a873-6c28906d29b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f93e77-dc3c-4cc0-90f4-31a4672c6907",
   "metadata": {},
   "source": [
    "### Loading XLNetTokenizer and building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258a98ff-a3fa-4e6f-b1e1-7ef256f47deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "def yield_tokens(train_iter):\n",
    "    for _,data_sample in train_iter:\n",
    "            yield tokenizer.tokenize(data_sample)\n",
    "vocab=build_vocab_from_iterator(yield_tokens(train_iter),specials=[\"<unk>\",\"<pad>\",'<|endoftext|>'],special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97270dfa-40e7-46ab-b6cb-1de9d6e1df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_string=vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be504598-fbf2-4a04-97f8-4564efea15a5",
   "metadata": {},
   "source": [
    "### Making of DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e565ee-a63a-4c2a-a5d8-ba5c8f9f7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Indices: [19, 1525, 19, 9566, 658, 6872, 9990, 3714, 22, 453]\n",
      "Target Indices: [1525, 19, 9566, 658, 6872, 9990, 3714, 22, 453, 4556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\rajve\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\rajve\\AppData\\Local\\Temp\\ipykernel_21712\\4171860680.py\", line 39, in <module>\n",
      "    src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
      "C:\\Users\\rajve\\AppData\\Local\\Temp\\ipykernel_21712\\4171860680.py:39: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  src_tensor = torch.tensor(src_indices, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "\n",
    "# 1. Final get_sample function\n",
    "def get_sample(block_size, tokens_list):\n",
    "    random_start = 0\n",
    "    stop = block_size\n",
    "    \n",
    "    # Slicing tokens\n",
    "    src_sequence = tokens_list[random_start:stop]\n",
    "    tgt_sequence = tokens_list[random_start + 1:stop + 1]\n",
    "    \n",
    "    # List conversion to avoid attribute errors and maintain alignment\n",
    "    src_sequence = list(src_sequence)\n",
    "    tgt_sequence = list(tgt_sequence)\n",
    "    \n",
    "    # Padding if target is short\n",
    "    if len(tgt_sequence) < len(src_sequence):\n",
    "        tgt_sequence.append('<|endoftext|>')\n",
    "        \n",
    "    return src_sequence, tgt_sequence\n",
    "\n",
    "# 2. Final implementation logic\n",
    "# train_iter se ek sample nikalna\n",
    "label, text = next(iter(train_iter))\n",
    "\n",
    "# IMPORTANT: .tokenize() use karein, sirf tokenizer(text) nahi\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Sequence generate karein\n",
    "block_size = 10\n",
    "src_text, tgt_text = get_sample(block_size, tokens)\n",
    "\n",
    "# 3. Numericalization (Ab error nahi aayega kyunki ye list of strings hai)\n",
    "src_indices = vocab(src_text)\n",
    "tgt_indices = vocab(tgt_text)\n",
    "\n",
    "# 4. Convert to Tensors\n",
    "src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "\n",
    "print(f\"Source Indices: {src_indices}\")\n",
    "print(f\"Target Indices: {tgt_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1e5588-3203-47c6-bb2d-1ae4a2aea405",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "batch_of_tokens=[]\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  _,text =next(iter(train_iter))\n",
    "  batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555ea38a-673f-433b-a783-ace84060282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=batch_of_tokens[0][0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a52fbd-db03-409b-86cc-aef83d0d6451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Source Sequence (Text): ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Source Sequence (Indices): [0, 0, 0]\n",
      "Source Sequence (Shape): torch.Size([3])\n",
      "Target Sequence (Text): ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Target Sequence (Indices): [0, 0, 0]\n",
      "Target Sequence (Shape): torch.Size([3])\n",
      "Sample 1:\n",
      "Source Sequence (Text): ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Source Sequence (Indices): [0, 0, 0]\n",
      "Source Sequence (Shape): torch.Size([3])\n",
      "Target Sequence (Text): ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Target Sequence (Indices): [0, 0, 0]\n",
      "Target Sequence (Shape): torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store source and target sequences\n",
    "src_batch, tgt_batch = [], []\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Loop to create batches of source and target sequences\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Retrieve the next data point from the training iterator\n",
    "    _,text = next(iter(train_iter))\n",
    "\n",
    "    # Generate source and target sequences using the get_sample function\n",
    "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
    "\n",
    "    # Convert source and target sequences to tokenized vocabulary indices\n",
    "    src_sequence_indices = vocab(src_sequence_text)\n",
    "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
    "\n",
    "    # Convert the sequences to PyTorch tensors with dtype int64\n",
    "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
    "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
    "\n",
    "    # Append the source and target sequences to their respective batches\n",
    "    src_batch.append(src_sequence)\n",
    "    tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    # Print the output for every 2nd sample (adjust as needed)\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
    "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
    "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
    "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
    "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
    "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "329b519d-001c-40a7-865f-b4e2b2e96030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(sample):\n",
    "    label, text = sample\n",
    "    tokens = tokenizer(text)\n",
    "    src_sequence, _ = get_sample(BLOCK_SIZE, tokens)\n",
    "    src_sequence = vocab(src_sequence)\n",
    "    text = torch.tensor(src_sequence, dtype=torch.long).unsqueeze(0)\n",
    "    label = torch.tensor(label, dtype=torch.long)\n",
    "    return text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d72de1a-d836-45e7-b1bd-ced163fb2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(train_iter)\n",
    "valid_list = list(valid_iter)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_list,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_list,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f0755-d395-4dd5-8a7f-53bc219fa0a3",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b336b145-76af-49de-b1da-e6805c9602c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(size):\n",
    "    mask=torch.tril((torch.ones(size,size))==1)\n",
    "    mask=mask.float().masked_fill(mask==0,float('-inf')).masked_fill(mask==1,float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bdac9d-4d8c-461d-96ef-04726fc14443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src):\n",
    "    seq_len=src.shape[0]\n",
    "    src_mask=generate_mask(seq_len)\n",
    "    src_padding_mask=(src==vocab(\"<pad>\")).transpose(0,1)\n",
    "    return src_mask,src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3756c86-628e-4820-9be5-8b709fe9cb0e",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfbea4f5-b0de-480f-9d5d-1fe85265fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,dropout):\n",
    "        super(Position_encoder,self).__init__()\n",
    "        pe=torch.zeros(vocab_size,d_model)\n",
    "        pos=torch.arange(0,vocab_size)\n",
    "        pos=pos.view(-1,1)\n",
    "        div_term=1/(10000**(2*torch.arange(0,d_model/2)/d_model))\n",
    "        div_term=div_term.view(1,-1)\n",
    "        pe[:,0::2]=torch.sin(pos*div_term)\n",
    "        pe[:,1::2]=torch.cos(pos*div_term)\n",
    "        self.pe=pe\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pe[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd2e86-4237-44fc-8941-7e790e608478",
   "metadata": {},
   "source": [
    "### Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612de7da-7300-40a4-be95-fc57a0e6e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276da92-1d7c-461a-8821-dc25bdf08abc",
   "metadata": {},
   "source": [
    "### Define GPT Like Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e652262a-bc99-40b6-9f7f-826f9a47ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Like_model(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_heads, num_layers, pad_idx, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.positional_encoder = Position_encoder(d_model=embed_size,vocab_size=vocab_size,dropout=dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        seq_len, batch_size = src.size()\n",
    "    \n",
    "        src_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=src.device) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )\n",
    "    \n",
    "        src_padding_mask = torch.zeros(batch_size, seq_len, device=src.device).bool()\n",
    "    \n",
    "        return src_mask, src_padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "    \n",
    "        seq_len, batch_size = x.size()\n",
    "    \n",
    "        src_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=x.device) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )\n",
    "    \n",
    "        x = self.embed(x)\n",
    "        x = self.positional_encoder(x)\n",
    "    \n",
    "        x = self.encoder(x, mask=src_mask)\n",
    "    \n",
    "        x = x.mean(dim=0)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851edaa3-c98e-4f0d-9810-5a8435757165",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "nlayers = 2\n",
    "nhead = 2  \n",
    "pad_idx=1\n",
    "dropout = 0.2\n",
    "model = GPT_Like_model(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,pad_idx=pad_idx,dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f60ec-b596-4b38-a467-ac342b7a5ab7",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07ca550-4bc0-4d82-8fc0-93e29c18c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=30\n",
    "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "    # Handle None prompt\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
    "\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Handle long prompts\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
    "\n",
    "    prompt_indices = vocab(tokens)\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "    return prompt_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5c44719-3cd0-41d5-b9f4-c19ddd6971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁prompt', '▁to', '▁get', '▁model', '▁generate', '▁next', '▁words', '.']\n"
     ]
    }
   ],
   "source": [
    "print([index_to_string[token] for token in encode_prompt(\n",
    "    \"This is a prompt to get model generate next words.\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8536b1b-a5f8-4ac3-8dd7-84cd04438c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#auto-regressive Language Model text generation\n",
    "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "\n",
    "    # Encode the input prompt using the provided encode_prompt function\n",
    "    prompt_encoded = encode_prompt(prompt)\n",
    "    tokens = []\n",
    "\n",
    "    # Generate new tokens up to max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Decode the encoded prompt using the model's decoder\n",
    "        logits = model(prompt_encoded)\n",
    "\n",
    "        # Transpose the logits to bring the sequence length to the first dimension\n",
    "        logits = logits.transpose(0, 1)\n",
    "\n",
    "        # Select the logits of the last token in the sequence\n",
    "        logit_prediction = logits[:, -1]\n",
    "\n",
    "        # Choose the most probable next token from the logits(greedy decoding)\n",
    "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "\n",
    "        # Convert the next token index to a token string using the vocabulary\n",
    "        # Move the tensor back to CPU for vocab lookup if needed\n",
    "        token_id = next_token_encoded.to('cpu').item()\n",
    "        tokens.append(vocab.get_itos()[token_id])\n",
    "\n",
    "    # Join the generated tokens into a single string and return\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ccb5e2f-e15f-4114-b82c-641de72f61c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁help ▁linebacker anov ▁presents ▁Faso ▁drunk ▁reverence ▁House lying ▁shook Land ▁Goddard jer ▁Neg ▁Avoid ▁goodness ▁mixture lick ▁lot ▁Resolution ▁maintain achi ewski ▁Nice pilot ▁Examples ▁Sen trans ▁deemed ito'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe0b3f-8b08-4ccb-ab8c-7551570bc899",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2247a0b5-fcf1-4153-9786-4a4158dbc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2944c88-8fb7-4f51-94dc-238695690900",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "\n",
    "def train(model: nn.Module,train_data) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data)) // block_size\n",
    "    for batch,srctgt in enumerate(train_data):\n",
    "        src= srctgt[0]\n",
    "        tgt= srctgt[1]\n",
    "        logits = model(src)\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            #cur_loss = total_loss / log_interval\n",
    "            cur_loss = total_loss / batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d68179-c4bc-4341-8f8d-be64691874a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "Train_losses = []\n",
    "Val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1), desc=\"Epochs\", position=0):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_iter, valid_iter = IMDB(split=(\"train\", \"test\"))\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "    for batch in tqdm(train_iter, desc=f\"Training Epoch {epoch}\", leave=False, position=1):\n",
    "        text, labels = collate_batch(batch)\n",
    "        text = text\n",
    "        labels = labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "\n",
    "    train_loss /= train_steps\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_iter, desc=f\"Validation Epoch {epoch}\", leave=False, position=2):\n",
    "            text, offsets, labels = collate_batch(batch)\n",
    "            text = text\n",
    "            offsets = offsets\n",
    "            labels = labels\n",
    "\n",
    "            output = model(text, offsets)\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    val_loss /= val_steps\n",
    "    val_ppl = math.exp(val_loss)\n",
    "\n",
    "    Train_losses.append(train_loss)\n",
    "    Val_losses.append(val_loss)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "\n",
    "    tqdm.write(\"-\" * 89)\n",
    "    tqdm.write(\n",
    "        f\"end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"train loss {train_loss:5.2f} | valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "    )\n",
    "    tqdm.write(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"model_best_val_loss.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
